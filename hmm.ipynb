{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "$$\n",
    "$$ $$\n",
    "$$\n",
    "$$\n",
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Hidden Markov Models (HMM)</div>\n",
    "\n",
    "This notebook aims to present a subject widely used in machine learning: Hidden Markov Models or HMM for short. While scikit-learn used to have a library specifically for their implementation, it has since been deprecated due to documentation and stability issues (the repository still exists and you can clone it using this [link](https://github.com/hmmlearn/hmmlearn)). Nonetheless, we will not be using it. The exercises and examples below should provide you with enough information to understand what a HMM is, what is happening underneath the hood and how parameter calibration is important to get the best results.\n",
    "\n",
    "__Dependencies__: _numpy_\n",
    "\n",
    "__Kernel__: Python 3\n",
    "\n",
     "\n",
    "__Author__: Gabriel Moreira 3\n",
    "\n",
    "To follow the notebook, it is recommended to read [Chapter A of the draft Speech and Language Processing by Daniel Jurafsky & James H. Martin](https://web.stanford.edu/~jurafsky/slp3/A.pdf) [1]. Some of its paragraphs and images are included in the notebook to help the reader but many algorithms were omitted to save time and space!\n",
    "\n",
    "1. [From Markov chains to HMM](#sec1)\n",
    "2. [Hidden Markov Model](#sec2)\n",
    "2. [Fundamental Problems Connected to HMM](#sec3)\n",
    "3. [Complete HMM](#sec4)\n",
    "4. [Defining Parameters and Convergence](#sec5)\n",
    "7. [To Finish Off](#sec6)\n",
    "6. [Going Further](#sec7)\n",
    "8. [References](#sec8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. <a id=\"sec1\"></a>From Markov chains to HMM \n",
    "\n",
    "## 1.1 Markov chains (MC)\n",
    "\n",
    "Suppose we want to model the weather at a certain location and we can use 3 labels: Hot, Warm and Cold. These represent the 3 possible states that a given day can be in and we'll represent them by $q_i$. Let us now consider the problem of forecasting a given day's weather supposing we have access to historical data.\n",
    "\n",
    "We can try to determine the following probability:\n",
    "\n",
    "$$\n",
    "P(q_i = a \\vert q_1 ... q_{i-1})\n",
    "$$\n",
    "\n",
    "Which can be translated into: whats the probability of the system being in the state $a$ (the day being Hot, for example), given that in the previous days a certain weather sequence was recorded. To simplify things, we can introduce what is called the Markov assumption. It states that the probability of the system transitioning to state $q_{t+1}$ depends only upon $q_{t}$, or more formally:\n",
    "\n",
    "$$\n",
    "P(q_i = a \\vert q_1 ... q_{i-1}) = P(q_i = a | q_{i-1})\n",
    "$$\n",
    "\n",
    "The image below represents examples of Markov chains:\n",
    "\n",
    "<img src=\"img/mc1.png\">\n",
    "\n",
    "## 1.2 Components of a MC\n",
    "1. A set of $N$ states (in our example Hot, Warm, Cold):\n",
    "\n",
    "$$\n",
    "Q = q_1,q_2,...,q_N\n",
    "$$\n",
    "\n",
    "2. The transition probability matrix where $a_{ij}$ represents the probability of state $i$ transitioning to state $j$:\n",
    "\n",
    "$$\n",
    "A =\\begin{bmatrix}\n",
    "    a_{11} &  \\dots  & a_{1N} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{N1} &  \\dots  & a_{NN}\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "\n",
    "3. Initial probability distribution over the $N$ states:\n",
    "\n",
    "$$\n",
    "\\pi = \\pi_1,\\pi_2,...,\\pi_N \n",
    "$$\n",
    "\n",
    "If, for instance $\\pi_0 = 0$, this means that the state $q_0$ cannot be an initial state.\n",
    "\n",
    "A Markov chain allows us to compute the probability of having, for example, 5 hot days in a row, provided the A matrix and the $\\pi$ vector are a good representation of the weather observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. <a id=\"sec2\"></a> Hidden Markov Model\n",
    "\n",
    "Let us now consider a system where we cannot directly infer the states, but what we can do instead is observe certain events that derive from those states. In speech recognition for instance, we can be interested in tagging words (with tags like: subject, verb, object, etc.). However what we can actually observe are the words themselves, so the tags can be considered hidden states. In similarity, the stock market goes through periodical regimes that have an influence on the volatility and the general direction of stock prices. Even though these regimes are not known directly, we can observe the stock prices and infer the regime sequence.\n",
    "\n",
    "## 2.1 Components of a HMM\n",
    "\n",
    "#### Components of a MC plus some other parameters\n",
    "Besides the $Q$ vector of $N$ states, the $\\pi$ vector with the initial probability distribution and the transition probability matrix $A$ defined previously we need to introduce the following additional components: \n",
    "\n",
    "1. A sequence of $T$ observations:\n",
    "$$\n",
    "O = o_1,o_2,...,o_T\n",
    "$$\n",
    "$$ $$\n",
    "2. A sequence of observation likelihoods (not necessarily probabilities but also called emission probabilities, for more information see the [difference between likelihood and probability)](https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability):\n",
    "\n",
    "$$\n",
    "B = b_i(o_t) \n",
    "$$\n",
    "\n",
    "The latter expresses the likelihood of an observation $o_t$ being generated while the system is in state $i$.\n",
    "\n",
    "In literature you will often find the following notation $\\lambda = (A,B, \\pi)$ to specify a HMM.\n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "A first-order hidden Markov model is based on two assumptions:\n",
    "\n",
    "1. The Markov assumption stated earlier:\n",
    "\n",
    "$$\n",
    "P(q_i = a \\vert q_1 ... q_{i-1}) = P(q_i = a | q_{i-1})\n",
    "$$\n",
    "\n",
    "2. Outcome independence, which means that the probability of observing $o_i$ depends only on the state responsible for producing the observation:\n",
    "\n",
    "$$\n",
    "P(o_i|q_q,...q_T, o_1,...o_T) = P(o_i|q_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us create a simple Markov model in Python that generates the $A$ matrix from the states provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Markov state\n",
    "'''\n",
    "class state:\n",
    "    def __init__(self, **kwargs):\n",
    "        # Defines the state's id\n",
    "        if 'id' in kwargs:\n",
    "            self.id = kwargs['id']\n",
    "\n",
    "        # State's probability\n",
    "        if 'p' in kwargs:\n",
    "            self.probability = kwargs['p']\n",
    "        else:\n",
    "            self.probability = 0.5 # default\n",
    "\n",
    "        # Transition dictionary\n",
    "        if 'transition' in kwargs:\n",
    "            self.transition = kwargs['transitions']\n",
    "        else:\n",
    "            self.transition = {}\n",
    "            \n",
    "        self.emission={}\n",
    "    \n",
    "    # Adds a transition from this state to toState with probability trans_prob\n",
    "    def addTransition(self, toState, trans_prob):\n",
    "        self.transition[toState.id] = trans_prob\n",
    "\n",
    "    # Adds observation/emission while in this state with probability emission_prob\n",
    "    def addEmission(self, observation, emission_prob):\n",
    "        self.emission[observation] = emission_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Markov process defined by states, transitions and observations\n",
    "'''\n",
    "class model:\n",
    "    def __init__(self, states, obs_seq):\n",
    "        self.states = states # list with all possible states\n",
    "        self.obs_seq = obs_seq # list with all possible observations\n",
    "        self.n = len(states) # number of states\n",
    "        self.pi = np.array([s.probability for s in states]) # initial probability vector\n",
    "        self.seq_len = len(obs_seq) # sequence length\n",
    "        \n",
    "        # Build A-matrix (state transition probabilities)\n",
    "        self.a = []\n",
    "        for thisState in self.states:\n",
    "            self.a.append([thisState.transition[thatState.id] for thatState in self.states])\n",
    "        self.a = np.matrix(self.a).reshape([self.n,self.n])\n",
    "        \n",
    "        # Build B-matrix (emission probabilities)\n",
    "        self.b = []\n",
    "        for currentState in self.states:\n",
    "            self.b.append([currentState.emission[obs] for obs in self.obs_seq])\n",
    "        self.b = np.matrix(self.b).reshape([self.n,self.seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather example\n",
    "\n",
    "To exemplify this model, imagine that you are a climatologist in the year 2799 studying the history of global warming. You cannot find any records of the weather in Toulouse, for the summer of 2018, but you do find a diary, which is listed how many ice creams were eaten every day that summer by one person. Our goal is to use these observations to estimate the temperature every day. Weâ€™ll simplify this by assuming there are only two kinds of days (or two states): Hot ($q_1$) and Cold ($q_2$). \n",
    "\n",
    "So the task is as follows:\n",
    "Given a sequence of observations $O$ (each of them an integer representing the\n",
    "number of ice creams eaten on a given day) find the â€˜hiddenâ€™ sequence\n",
    "$Q$ of weather states ($q_1$ or $q_2$) which caused that person to eat the ice cream.\n",
    "\n",
    "<img src=\"img/hmm1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the HMM represented in the image above using the state and model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Defining the states \n",
    "'''\n",
    "q1 = state(id='hot', p=0.8)\n",
    "q2 = state(id='cold', p=0.2)\n",
    "q1.addTransition(q1, 0.6)\n",
    "q1.addTransition(q2, 0.4)\n",
    "q2.addTransition(q1, 0.5)\n",
    "q2.addTransition(q2, 0.5)\n",
    "q1.addEmission('1', 0.2)\n",
    "q1.addEmission('2', 0.4)\n",
    "q1.addEmission('3', 0.4)\n",
    "q2.addEmission('1', 0.5)\n",
    "q2.addEmission('2', 0.4)\n",
    "q2.addEmission('3', 0.1)\n",
    "\n",
    "'''\n",
    "    Creating the model\n",
    "'''\n",
    "mdl = model([q1,q2], ['3', '1', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State transition matrix A:\n",
      "[[0.6 0.4]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "print('State transition matrix A:')\n",
    "print(mdl.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id=\"sec3\"></a>  Fundamental problems connected to HMM\n",
    "\n",
    "An influential tutorial by [Rabiner (1989)](https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf) [2], introduced the idea that hidden Markov models should be characterized\n",
    "by three fundamental problems:\n",
    "\n",
    "1. __Likelihood__ - For a given HMM $\\lambda = (A,B)$ and an observation sequence $O=o_1,...o_T$, calculate the likelihood $P(O|\\lambda)$;\n",
    "2. __Decoding__ - For a given sequence $O$ and model $\\lambda$ find the most likely (hidden) state sequence $Q$;\n",
    "3. __Learning__ - Given an observation sequence $O$ and a set of states, learn the parameters $A$ and $B$ that define the HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Forward algorithm\n",
    "\n",
    "\n",
    "\n",
    "Our first problem is to compute the likelihood of a particular observation sequence. For example, given the ice-cream eating HMM in Fig. A.2, what is the probability of the sequence 3 1 3? For a Markov chain, where the surface observations are the same as the hidden events, we could compute the probability of 3 1 3 just by following the states labeled 3 1 3 and multiplying the probabilities along the arcs. For a hidden Markov model, things are not so simple.\n",
    "The forward algorithm is a kind of dynamic programming algorithm, that is, an algorithm that uses a table to store\n",
    "intermediate values as it builds up the probability of the observation sequence. The forward algorithm computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a single forward trellis. This trellis is represented in the images below:\n",
    "\n",
    "<img src=\"img/hmm2.png\">\n",
    "<img src=\"img/hmm3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn!\n",
    "\n",
    "Using the trellis represented in the previous image, create the _\\_forward_\\_ function that takes in a model object or the matrices $A$, $B$ and the vector $\\pi$ and outputs the $\\alpha$ matrix. You can find the whole algorithm in [1]. When you are done, feed your function with the data from the ice-cream example and check the output! \n",
    "\n",
    "If you want to skip this step just use the _forward_ function already coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward_(A, B, pi):\n",
    "\n",
    "    '''\n",
    "    \n",
    "        Your _forward_ function goes here:\n",
    "        1) Initialization        \n",
    "        2) Recursion\n",
    "        4) Termination\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your code with the following, and check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, verbose):\n",
    "    if verbose:\n",
    "        print('\\n>>> Forward algorithm. Calculating...')\n",
    "\n",
    "    edges = []\n",
    "        \n",
    "    # Initialization\n",
    "    alpha = np.multiply(model.pi, [state.emission[model.obs_seq[0]]\n",
    "                                   for state in model.states]).reshape([-1,1])\n",
    "\n",
    "    # Recursion\n",
    "    for j in range(1,model.seq_len):\n",
    "        # Generates the matrix with the edge values\n",
    "        edge_matrix = []\n",
    "        \n",
    "        for k in range(model.n):\n",
    "            row = [model.a[i,k]*model.b[k,j] for i in range(model.n)]\n",
    "            edge_matrix.append(row)\n",
    "            \n",
    "        edge_matrix = np.matrix(edge_matrix).reshape([model.n,model.n])\n",
    "        edges.append(edge_matrix)\n",
    "\n",
    "        # Adds the next column of the alpha matrix\n",
    "        next_col = np.matmul(edge_matrix, alpha[:,j-1].reshape([-1,1]))\n",
    "        alpha = np.concatenate((alpha, next_col), axis=1)\n",
    "\n",
    "    # Termination\n",
    "    obs_probability = np.sum(alpha[:,model.seq_len-1])\n",
    "\n",
    "    if verbose:\n",
    "        print('\\n.Alpha-matrix:')\n",
    "        print(alpha)\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Forward algorithm. Calculating...\n",
      "\n",
      ".Alpha-matrix:\n",
      "[[0.32     0.0404   0.023496]\n",
      " [0.02     0.069    0.005066]]\n"
     ]
    }
   ],
   "source": [
    "alpha = forward(mdl, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Decoding\n",
    "\n",
    "For any model, such as an HMM, that contains hidden variables, the task of determining which sequence of variables is the underlying source of some sequence of observations is called the decoding task. In the ice-cream domain, given a sequence of ice-cream observations 3 1 3 and an HMM, the task of the decoder is to find the best hidden weather sequence. The most common decoding algorithms for HMMs is the _Viterbi_ algorithm.\n",
    "\n",
    "<img src=\"img/hmm4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn!\n",
    "\n",
    "Using the trellis represented in the previous image, create the  _\\_decoder_\\_ function that takes in the model or the matrices A, B and the vector $\\pi$ and outputs the most likely state sequence. Once again, you can find the whole Viterbi algorithm in [1]!\n",
    "\n",
    "... Or skip ahead and use the _decoder_ function already coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decoder_(A, B, pi):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "        Your decoder function goes here:\n",
    "        \n",
    "        \n",
    "        \n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your state sequence with the one generate from the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Viterbi decoder - from a sequence of observations infer the most probable\n",
    "    hidden state sequence.\n",
    "'''\n",
    "def decoder(model, verbose):\n",
    "    if verbose:\n",
    "        print('\\n>>> Viterbi decoder. Calculating...')\n",
    "\n",
    "    # First Viterbi matrix column\n",
    "    v = np.multiply(model.pi, [model.b[k,0] for k in range(model.n)]).reshape([-1,1])\n",
    "\n",
    "\n",
    "    # Keep track of the index of the most likely state\n",
    "    index_backtrack = [np.where(v == np.max(v))[0][0]]\n",
    "\n",
    "    for j in range(1,model.seq_len):\n",
    "        \n",
    "        # Generates the matrix with the edge values\n",
    "        edge_matrix = []\n",
    "        for k in range(model.n):\n",
    "            row = [model.a[i,k] * model.b[k,j] for i in range(model.n)]\n",
    "            edge_matrix.append(row)\n",
    "            \n",
    "        edge_matrix = np.matrix(edge_matrix).reshape([model.n,model.n])\n",
    "\n",
    "        likely_state_index = np.where(edge_matrix[:,index_backtrack[j-1]] ==\n",
    "                             np.max(edge_matrix[:,index_backtrack[j-1]]))[0][0]\n",
    "\n",
    "        index_backtrack.append(likely_state_index)\n",
    "\n",
    "        # Adds the next column of the viterbi matrix\n",
    "        prev_col = v[:,j-1].reshape([1,-1])\n",
    "        next_col = np.array([np.max(np.multiply(edge_matrix[i,:], prev_col))\n",
    "                             for i in range(model.n)]).reshape([-1,1])\n",
    "        v = np.concatenate((v, next_col), axis=1)\n",
    "\n",
    "        state_seq = [model.states[i].id for i in index_backtrack]\n",
    "        \n",
    "    if verbose:\n",
    "        print('\\n.Viterbi matrix:')\n",
    "        print(v)\n",
    "        print('\\n.Most likely state sequence:')\n",
    "        print(state_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Viterbi decoder. Calculating...\n",
      "\n",
      ".Viterbi matrix:\n",
      "[[0.32   0.0384 0.0128]\n",
      " [0.02   0.064  0.0032]]\n",
      "\n",
      ".Most likely state sequence:\n",
      "['hot', 'cold', 'hot']\n"
     ]
    }
   ],
   "source": [
    "decoder(mdl, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. <a id=\"sec4\"></a> Complete HMM class in Python \n",
    "\n",
    "\n",
    "Now let us put it all together in a model class and include the $fit()$ method which will train the model, updating the matrices $A$ and $B$, taking into account the observation sequence provided. The fit method implements the Baum-Welch (or forward backward) algorithm using the forward function you coded earlier. However, is not provided as an exercise due to its length. It is explained in full detail in [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class model:\n",
    "    '''\n",
    "        Initializes a HMM with an observation sequence: obs_seq\n",
    "        and a list of states: states\n",
    "    '''\n",
    "    def __init__(self, obs_seq, states, **kwargs):   \n",
    "        self.states = states\n",
    "        self.n = len(self.states) # number of states\n",
    "        self.obs_seq = obs_seq # list with all possible observations\n",
    "        self.pi = np.array([s.probability for s in self.states]) # initial probability vector\n",
    "        self.seq_len = len(obs_seq)\n",
    "        \n",
    "        if 'n_symbols' in kwargs:\n",
    "            self.n_symbols = kwargs['n_symbols']\n",
    "            \n",
    "        # Build A-matrix\n",
    "        self.a = []\n",
    "        for thisState in self.states:\n",
    "            self.a.append([thisState.transition[thatState.id] \n",
    "                           for thatState in self.states])\n",
    "        self.a = np.matrix(self.a).reshape([self.n,self.n])\n",
    "\n",
    "        # Build B-matrix\n",
    "        self.b = []\n",
    "        for currentState in self.states:\n",
    "            self.b.append([currentState.emission[obs] for obs in self.obs_seq])\n",
    "        self.b = np.matrix(self.b).reshape([self.n,self.seq_len])\n",
    "            \n",
    "    '''\n",
    "        Forward algorithm:\n",
    "        Calculates the probability of a given observation sequence\n",
    "        Generates the alpha-matrix\n",
    "    '''\n",
    "    def forward(self, verbose):\n",
    "        if verbose:\n",
    "            print('\\n>>> Forward algorithm. Calculating...')\n",
    "\n",
    "        edges = []\n",
    "    \n",
    "        # Initialization\n",
    "        self.alpha = np.matrix(np.multiply(self.pi, self.b[:,0].reshape([1,-1]).tolist()[0]).reshape([-1,1]))\n",
    "        \n",
    "        # Recursion\n",
    "        for j in range(1,self.seq_len):\n",
    "            # Generates the matrix with the edge values\n",
    "            edge_matrix = []\n",
    "            for k in range(self.n):\n",
    "                row = [self.a[i,k]*self.b[k,j] for i in range(self.n)]\n",
    "                edge_matrix.append(row)\n",
    "            edge_matrix = np.matrix(edge_matrix).reshape([self.n,self.n])\n",
    "            edges.append(edge_matrix)\n",
    "\n",
    "            # Adds the next column of the alpha matrix\n",
    "            next_col = np.matmul(edge_matrix, self.alpha[:,j-1].reshape([-1,1]))\n",
    "            self.alpha = np.concatenate((self.alpha, next_col), axis=1)\n",
    "\n",
    "        # Termination\n",
    "        self.obs_probability = np.sum(self.alpha[:,self.seq_len-1])\n",
    "\n",
    "        if verbose:\n",
    "            print('\\n.Alpha-matrix:')\n",
    "            print(self.alpha)\n",
    "\n",
    "        return self.alpha\n",
    "\n",
    "    '''\n",
    "        Viterbi decoder - from a sequence of observations infer the most probable\n",
    "        hidden state sequence.\n",
    "    '''\n",
    "    def decoder(self, verbose):\n",
    "        if verbose:\n",
    "            print('\\n>>> Viterbi decoder. Calculating...')\n",
    "\n",
    "        # First Viterbi matrix column\n",
    "        v = np.matrix(np.multiply(self.pi, self.b[:,0].reshape([1,-1]).tolist()[0]).reshape([-1,1]))\n",
    "\n",
    "\n",
    "        # Keep track of the index of the most likely state\n",
    "        index_backtrack = [np.where(v == np.max(v))[0][0]]\n",
    "\n",
    "        for j in range(1,self.seq_len):\n",
    "            # Generates the matrix with the edge values\n",
    "            edge_matrix = []\n",
    "            for k in range(self.n):\n",
    "                row = [self.a[i,k] * self.b[k,j] for i in range(self.n)]\n",
    "                edge_matrix.append(row)\n",
    "            edge_matrix = np.matrix(edge_matrix).reshape([self.n,self.n])\n",
    "\n",
    "            likely_state_index = np.where(edge_matrix[:,index_backtrack[j-1]] ==\n",
    "                                 np.max(edge_matrix[:,index_backtrack[j-1]]))[0][0]\n",
    "\n",
    "            index_backtrack.append(likely_state_index)\n",
    "\n",
    "            # Adds the next column of the viterbi matrix\n",
    "            prev_col = v[:,j-1].reshape([1,-1])\n",
    "            next_col = np.array([np.max(np.multiply(edge_matrix[i,:], prev_col))\n",
    "                       for i in range(self.n)]).reshape([-1,1])\n",
    "            v = np.concatenate((v, next_col), axis=1)\n",
    "\n",
    "        state_seq = [self.states[i].id for i in index_backtrack]\n",
    "        if verbose:\n",
    "            print('\\n.Viterbi matrix:')\n",
    "            print(v)\n",
    "            print('\\n.Most likely state sequence:')\n",
    "            print(state_seq)\n",
    "\n",
    "    '''\n",
    "        Computes the Beta-matrix using the backward algorithm\n",
    "    '''\n",
    "    def backward(self, verbose):\n",
    "        if verbose:\n",
    "            print('\\n>>> Backward probability. Calculating...')\n",
    "\n",
    "        # Initialization\n",
    "        self.beta = np.zeros([self.n, self.seq_len])\n",
    "        self.beta[:,self.seq_len-1] = np.ones(self.n)\n",
    "\n",
    "        # Recursion\n",
    "        for j in range(1,self.seq_len):\n",
    "            for i in range(self.n):\n",
    "                self.beta[:,self.seq_len-1-j] += self.beta[i,self.seq_len-j] * \\\n",
    "                                                 np.array(self.a[:,i])[0] * \\\n",
    "                                                 self.b[i,self.seq_len-j]\n",
    "        self.beta = np.matrix(self.beta)\n",
    "\n",
    "        if verbose:\n",
    "            print('\\n.Beta matrix:')\n",
    "            print(self.beta)\n",
    "\n",
    "        return self.beta\n",
    "\n",
    "    '''\n",
    "        Train the HMM - computing the state transition and emission probabilities\n",
    "        based on the forward-backward (Baum-Welch) algorithm\n",
    "    '''\n",
    "    def fit(self, verbose, max_iter):\n",
    "        if verbose:\n",
    "            print('\\n>>> Training HMM...')\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            self.forward(verbose=False)\n",
    "            self.backward(verbose=False)\n",
    "\n",
    "            # Calculate Xi[t,i,j] matrix\n",
    "            xi = []\n",
    "            for t in range(self.seq_len-1):\n",
    "                num_t = np.zeros([self.n,self.n])\n",
    "                for i in range(self.n):\n",
    "                    for j in range(self.n):\n",
    "                        alpha_ti = self.alpha[i,t]\n",
    "                        a_ij = self.a[i,j]\n",
    "                        b_jt1 = self.b[j,t+1]\n",
    "                        beta_jt1 = self.beta[j,t+1]\n",
    "                        num_t[i,j] = alpha_ti*a_ij*b_jt1*beta_jt1\n",
    "                xi.append(np.matrix(num_t))\n",
    "                xi[t] /= np.sum(np.sum(num_t,axis=0))\n",
    "                \n",
    "                \n",
    "            # Calculate gamma matrix\n",
    "            gamma = [np.sum(xi[i], axis=1) for i in range(len(xi))]\n",
    "            \n",
    "            # Calculate a-hat matrix\n",
    "            a_hat_num = np.sum(xi,axis=0)\n",
    "            a_hat_den = np.array(np.sum(gamma, axis=0)).reshape([1,-1])[0]\n",
    "            a_hat = np.matrix(np.zeros([self.n, self.n]))\n",
    "            for i in range(self.n):\n",
    "                for j in range(self.n):\n",
    "                    a_hat[i,j] = a_hat_num[i,j] / a_hat_den[i]\n",
    "        \n",
    "            # Calculate probability of symbol k while in a certain state\n",
    "            symbol_prob = np.matrix(np.zeros([self.n, self.n_symbols]))\n",
    "            for k in range(1,self.n_symbols+1):\n",
    "                v_k = str(k)\n",
    "                for kk in range(self.seq_len-1):\n",
    "                    if str(self.obs_seq[kk]) == v_k:\n",
    "                        symbol_prob[:,k-1] += gamma[kk]\n",
    "            \n",
    "            # Calculate b-hat matrix\n",
    "            b_hat_num = np.matrix(np.zeros([self.n, self.seq_len]))\n",
    "            b_hat = b_hat_num\n",
    "            for t in range(self.seq_len):\n",
    "                for k in range(1,self.n_symbols+1):\n",
    "                    for j in range(self.n):\n",
    "                        if self.obs_seq[t] == str(k):\n",
    "                            b_hat_num[j,t] = symbol_prob[j,k-1] / a_hat_den[j]\n",
    "                        \n",
    "            # Update matrices\n",
    "            self.a = a_hat\n",
    "            self.b = b_hat\n",
    "\n",
    "            '''\n",
    "            If you want to see the probability of the sequence \n",
    "            converging as the training process goes on\n",
    "            UNCOMMENT LINE BELOW!\n",
    "            '''\n",
    "            print(self.obs_probability)\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\n.State transitions probabilities estimates (A-hat matrix):')\n",
    "            print(self.a)\n",
    "            print('\\n.Emission probabilities estimates (B-hat matrix):')\n",
    "            print(self.b)\n",
    "            print('\\n.Sequence probability:')\n",
    "            print(self.obs_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id=\"sec5\"></a> Defining parameters and verifying convergence\n",
    "\n",
    "You can now use the class defined above and its methods to play with the initial parameters ($\\pi$, $A$ and $B$) and the observation sequence to see how the results vary. Use the code below as a starting example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".A-matrix before fit\n",
      "[[0.7 0.3]\n",
      " [0.3 0.7]]\n",
      "\n",
      ".B-matrix before fit\n",
      "[[0.5 0.5 0.5 0.4 0.4 0.1 0.1 0.1 0.1 0.1 0.4 0.5 0.4 0.5 0.5 0.5]\n",
      " [0.1 0.1 0.1 0.2 0.2 0.7 0.7 0.7 0.7 0.7 0.2 0.1 0.2 0.1 0.1 0.1]]\n",
      "9.956492805475383e-08\n",
      "5.067808196951614e-07\n",
      "8.825428264116487e-07\n",
      "8.962858080926481e-07\n",
      "8.664346068270807e-07\n",
      "8.597676890479551e-07\n",
      "8.647896979754734e-07\n",
      "8.761080386332066e-07\n",
      "8.916103254399431e-07\n",
      "9.0982380705969e-07\n",
      "9.295012810873849e-07\n",
      "9.496408125663863e-07\n",
      "9.694853377786605e-07\n",
      "9.884930949947306e-07\n",
      "1.006302758305215e-06\n",
      "1.0226996494718184e-06\n",
      "1.037583690224978e-06\n",
      "1.0509399275342271e-06\n",
      "1.0628128734569957e-06\n",
      "1.0732855323308252e-06\n",
      "1.0824632759099286e-06\n",
      "1.090462119171787e-06\n",
      "1.097400621905329e-06\n",
      "1.1033945711606157e-06\n",
      "1.1085536880097225e-06\n",
      "1.1129797536056285e-06\n",
      "1.1167657032623204e-06\n",
      "1.1199953656890108e-06\n",
      "1.1227436215084102e-06\n",
      "1.125076824640217e-06\n",
      "1.1270533786707377e-06\n",
      "1.12872439406628e-06\n",
      "1.1301343756516058e-06\n",
      "1.1313219063750583e-06\n",
      "1.1323203051669659e-06\n",
      "1.1331582450957247e-06\n",
      "1.133860324000533e-06\n",
      "1.1344475839948785e-06\n",
      "1.1349379791614124e-06\n",
      "1.1353467927371004e-06\n",
      "1.1356870063689261e-06\n",
      "1.1359696247928334e-06\n",
      "1.1362039596938372e-06\n",
      "1.1363978766495353e-06\n",
      "1.1365580090237462e-06\n",
      "1.136689942521315e-06\n",
      "1.1367983738842573e-06\n",
      "1.1368872469359947e-06\n",
      "1.136959868887495e-06\n",
      "1.1370190095237189e-06\n",
      "1.1370669856015387e-06\n",
      "1.1371057325184624e-06\n",
      "1.1371368650595313e-06\n",
      "1.1371617287998138e-06\n",
      "1.1371814435321275e-06\n",
      "1.1371969399046154e-06\n",
      "1.1372089902886624e-06\n",
      "1.137218234753338e-06\n",
      "1.1372252028967574e-06\n",
      "1.1372303321747505e-06\n",
      "1.1372339832729113e-06\n",
      "1.1372364529858438e-06\n",
      "1.1372379849973576e-06\n",
      "1.1372387788952094e-06\n",
      "1.137238997702452e-06\n",
      "1.137238774163553e-06\n",
      "1.1372382159864325e-06\n",
      "1.1372374102094825e-06\n",
      "1.137236426836315e-06\n",
      "1.1372353218578244e-06\n",
      "1.1372341397622534e-06\n",
      "1.1372329156177451e-06\n",
      "1.137231676798081e-06\n",
      "1.1372304444109785e-06\n",
      "1.1372292344785853e-06\n",
      "1.13722805891173e-06\n",
      "1.1372269263126095e-06\n",
      "1.1372258426349743e-06\n",
      "1.1372248117259932e-06\n",
      "1.1372238357700381e-06\n",
      "1.1372229156511616e-06\n",
      "1.1372220512484084e-06\n",
      "1.1372212416755104e-06\n",
      "1.1372204854748102e-06\n",
      "1.1372197807733801e-06\n",
      "1.1372191254080972e-06\n",
      "1.137218517025267e-06\n",
      "1.137217953159328e-06\n",
      "1.1372174312945287e-06\n",
      "1.1372169489126792e-06\n",
      "1.1372165035296507e-06\n",
      "1.1372160927227343e-06\n",
      "1.137215714150617e-06\n",
      "1.1372153655674916e-06\n",
      "1.1372150448324202e-06\n",
      "1.1372147499150185e-06\n",
      "1.1372144788982135e-06\n",
      "1.1372142299786777e-06\n",
      "1.137214001465628e-06\n",
      "1.1372137917782186e-06\n",
      "\n",
      ".A-matrix after fit\n",
      "[[0.9000028  0.0999972 ]\n",
      " [0.19994853 0.80005147]]\n",
      "\n",
      ".B-matrix after fit\n",
      "[[6.00008232e-01 6.00008232e-01 6.00008232e-01 3.99991768e-01\n",
      "  3.99991768e-01 1.73984080e-69 1.73984080e-69 1.73984080e-69\n",
      "  1.73984080e-69 1.73984080e-69 3.99991768e-01 6.00008232e-01\n",
      "  3.99991768e-01 6.00008232e-01 6.00008232e-01 6.00008232e-01]\n",
      " [2.13264091e-91 2.13264091e-91 2.13264091e-91 2.74397903e-05\n",
      "  2.74397903e-05 9.99972560e-01 9.99972560e-01 9.99972560e-01\n",
      "  9.99972560e-01 9.99972560e-01 2.74397903e-05 2.13264091e-91\n",
      "  2.74397903e-05 2.13264091e-91 2.13264091e-91 2.13264091e-91]]\n",
      "\n",
      ">>> Viterbi decoder. Calculating...\n",
      "\n",
      ".Viterbi matrix:\n",
      "[[3.00004116e-01 1.62004949e-01 8.74841452e-02 3.14937421e-02\n",
      "  1.13375491e-02 1.77530326e-71 3.94387215e-73 3.15521413e-73\n",
      "  2.52426444e-73 2.01948606e-73 3.71439779e-05 2.00580857e-05\n",
      "  7.22078470e-06 3.89928937e-06 2.10565170e-06 1.13707106e-06]\n",
      " [1.06632046e-91 6.39783137e-93 3.45488709e-93 2.40047938e-07\n",
      "  8.64157479e-08 1.13369205e-03 9.06987105e-04 7.25616456e-04\n",
      "  5.80514583e-04 4.64428801e-04 1.01957190e-08 7.92125489e-97\n",
      "  5.50374253e-11 1.53989097e-97 8.31555119e-98 4.49047322e-98]]\n",
      "\n",
      ".Most likely state sequence:\n",
      "['hot', 'hot', 'hot', 'hot', 'hot', 'cold', 'cold', 'cold', 'cold', 'cold', 'hot', 'hot', 'hot', 'hot', 'hot', 'hot']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Defining the states \n",
    "'''\n",
    "q1 = state(id='hot', p=0.5)\n",
    "q2 = state(id='cold', p=0.5)\n",
    "q1.addTransition(q1, 0.7)\n",
    "q1.addTransition(q2, 0.3)\n",
    "q2.addTransition(q1, 0.3)\n",
    "q2.addTransition(q2, 0.7)\n",
    "q1.addEmission('1', 0.1)\n",
    "q1.addEmission('2', 0.4)\n",
    "q1.addEmission('3', 0.5)\n",
    "q2.addEmission('1', 0.7)\n",
    "q2.addEmission('2', 0.2)\n",
    "q2.addEmission('3', 0.1)\n",
    "\n",
    "'''\n",
    "    Creating the model\n",
    "'''\n",
    "sequence = ['3', '3', '3', '2', '2', '1', '1', '1', '1', '1', '2', '3', '2', '3', '3', '3']\n",
    "mdl = model(sequence, [q1,q2], n_symbols=3)\n",
    "\n",
    "print('\\n.A-matrix before fit')\n",
    "print(mdl.a)\n",
    "\n",
    "print('\\n.B-matrix before fit')\n",
    "print(mdl.b)\n",
    "\n",
    "# updates A and B matrices using the observation sequence fed to the model\n",
    "mdl.fit(max_iter=100, verbose=False)\n",
    "\n",
    "print('\\n.A-matrix after fit')\n",
    "print(mdl.a)\n",
    "\n",
    "print('\\n.B-matrix after fit')\n",
    "print(mdl.b)\n",
    "\n",
    "mdl.decoder(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the A and B matrices are different after the fit. And you should see that the initial parameters change these matrices even after the fit is performed. Try it yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn!\n",
    "\n",
    "What happens in the following cases?\n",
    "\n",
    "1. The input observation sequence is too long. Try it with arrays as long as +1000 elements. Uncomment the line __print(self.obs_probability)__ in the _fit_ method of the class _model_ to check if the observation sequence probability converges.\n",
    "2. You change the input parameters. Does the _Viterbi_ algorithm have the same output? What about the observation probability?\n",
    "3. You add more states. Does it always converge?\n",
    "\n",
    "Use the code below as an example and feel free to play around. Plot the observation sequence probability for different initial parameters. Check if it convergences. Ajust the maximum number of iterations, __max_iter__ if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training HMM...\n",
      "4.764216432640001e-06\n",
      "6.478029155013581e-06\n",
      "7.105096949440317e-06\n",
      "7.493634965234551e-06\n",
      "7.7454720549113e-06\n",
      "7.900974263087925e-06\n",
      "7.987036647867046e-06\n",
      "8.024180866834028e-06\n",
      "8.027625845316842e-06\n",
      "8.00826903888329e-06\n",
      "7.973754792855237e-06\n",
      "7.929387916404646e-06\n",
      "7.878827249028827e-06\n",
      "7.824585107982434e-06\n",
      "7.768379643616661e-06\n",
      "7.711382086429203e-06\n",
      "7.654390141659947e-06\n",
      "7.597949406504043e-06\n",
      "7.542437960417899e-06\n",
      "7.4881247251779465e-06\n",
      "7.4352090967977665e-06\n",
      "7.3838471985754105e-06\n",
      "7.334168578993188e-06\n",
      "7.286286093516503e-06\n",
      "7.240300947243692e-06\n",
      "7.196304352598537e-06\n",
      "7.154376908690185e-06\n",
      "7.114586583885386e-06\n",
      "7.076986035478527e-06\n",
      "7.041609891171113e-06\n",
      "7.00847251421067e-06\n",
      "6.977566654409315e-06\n",
      "6.948863240051946e-06\n",
      "6.922312394697042e-06\n",
      "6.897845585899347e-06\n",
      "6.875378657184747e-06\n",
      "6.854815388432397e-06\n",
      "6.836051192663673e-06\n",
      "6.818976592461827e-06\n",
      "6.803480212269468e-06\n",
      "6.789451145875005e-06\n",
      "6.776780679853601e-06\n",
      "6.7653634478344136e-06\n",
      "6.755098143811278e-06\n",
      "6.745887935332481e-06\n",
      "6.737640699320625e-06\n",
      "6.730269168808017e-06\n",
      "6.723691041412651e-06\n",
      "6.7178290691603516e-06\n",
      "6.712611128529995e-06\n",
      "6.707970259518755e-06\n",
      "6.703844660788561e-06\n",
      "6.700177631394662e-06\n",
      "6.696917455274662e-06\n",
      "6.694017230453976e-06\n",
      "6.691434649550811e-06\n",
      "6.68913174114915e-06\n",
      "6.687074582970224e-06\n",
      "6.685232997802274e-06\n",
      "6.683580242244518e-06\n",
      "6.682092696863024e-06\n",
      "6.6807495646590645e-06\n",
      "6.679532583043378e-06\n",
      "6.67842575293598e-06\n",
      "6.6774150872526665e-06\n",
      "6.6764883799224585e-06\n",
      "6.675634995709104e-06\n",
      "6.674845680453231e-06\n",
      "6.674112390891173e-06\n",
      "6.6734281428972e-06\n",
      "6.672786876812914e-06\n",
      "6.672183338438223e-06\n",
      "6.671612974239957e-06\n",
      "6.671071839363535e-06\n",
      "6.670556517098864e-06\n",
      "6.670064048534868e-06\n",
      "6.669591871236455e-06\n",
      "6.669137765877466e-06\n",
      "6.668699809866751e-06\n",
      "6.6682763371023125e-06\n",
      "6.667865903083293e-06\n",
      "6.667467254695283e-06\n",
      "6.667079304064985e-06\n",
      "6.666701105952962e-06\n",
      "6.666331838217949e-06\n",
      "6.6659707849445486e-06\n",
      "6.665617321878523e-06\n",
      "6.665270903858789e-06\n",
      "6.6649310539769765e-06\n",
      "6.6645973542293325e-06\n",
      "6.664269437457909e-06\n",
      "6.663946980404867e-06\n",
      "6.663629697727004e-06\n",
      "6.663317336838843e-06\n",
      "6.663009673469872e-06\n",
      "6.662706507837437e-06\n",
      "6.662407661350324e-06\n",
      "6.662112973769126e-06\n",
      "6.661822300760861e-06\n",
      "6.661535511791777e-06\n",
      "\n",
      ".State transitions probabilities estimates (A-hat matrix):\n",
      "[[0.69297186 0.30702814]\n",
      " [0.34426026 0.65573974]]\n",
      "\n",
      ".Emission probabilities estimates (B-hat matrix):\n",
      "[[3.98999085e-03 3.98999085e-03 6.06793309e-01 6.06793309e-01\n",
      "  3.89216700e-01 6.06793309e-01 3.89216700e-01 3.98999085e-03\n",
      "  3.89216700e-01 3.89216700e-01 3.98999085e-03]\n",
      " [5.89455474e-01 5.89455474e-01 3.81941157e-16 3.81941157e-16\n",
      "  4.10544526e-01 3.81941157e-16 4.10544526e-01 5.89455474e-01\n",
      "  4.10544526e-01 4.10544526e-01 5.89455474e-01]]\n",
      "\n",
      ".Sequence probability:\n",
      "6.661535511791777e-06\n",
      "\n",
      ">>> Viterbi decoder. Calculating...\n",
      "\n",
      ".Viterbi matrix:\n",
      "[[1.99499543e-03 4.04836635e-04 2.37974924e-02 1.00066239e-02\n",
      "  2.69894875e-03 1.13488281e-03 3.06096300e-04 8.46341383e-07\n",
      "  7.42276373e-06 2.00203977e-06 5.53554258e-09]\n",
      " [2.94727737e-01 1.13920929e-01 2.85319516e-17 2.79065304e-18\n",
      "  1.26132217e-03 3.16496773e-19 1.43050530e-04 5.53971311e-05\n",
      "  1.49134816e-05 4.01486373e-06 1.55186278e-06]]\n",
      "\n",
      ".Most likely state sequence:\n",
      "['cold', 'cold', 'hot', 'hot', 'hot', 'hot', 'hot', 'cold', 'cold', 'cold', 'cold']\n"
     ]
    }
   ],
   "source": [
    "q1 = state(id='hot', p=0.5) # changed the initial probability\n",
    "q2 = state(id='cold', p=0.5) # changed the initial probability\n",
    "\n",
    "# Transitions probabilities are now uniform and all equal to 0.5\n",
    "q1.addTransition(q1, 0.9)\n",
    "q1.addTransition(q2, 0.1)\n",
    "q2.addTransition(q1, 0.3)\n",
    "q2.addTransition(q2, 0.7)\n",
    "\n",
    "# Different emission probabilities\n",
    "q1.addEmission('1', 0.1)\n",
    "q1.addEmission('2', 0.4)\n",
    "q1.addEmission('3', 0.5)\n",
    "q2.addEmission('1', 0.7)\n",
    "q2.addEmission('2', 0.2)\n",
    "q2.addEmission('3', 0.1)\n",
    "\n",
    "sequence = ['1', '1', '3', '3', '2', '3', '2', '1', '2', '2', '1'] # Try long sequences\n",
    "mdl = model(sequence, [q1,q2], n_symbols=3)\n",
    "mdl.fit(max_iter=100, verbose=True) # Use verbose=True to see the calculations\n",
    "mdl.decoder(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n    Your own sandbox\\n    Use the model class to create your own HMM\\n    Test it!\\n    \\n    \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "    Your own sandbox\n",
    "    Use the model and state classes to create your own HMM\n",
    "    Test it!\n",
    "    \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note to the reader:__ You should notice that initial parameters play an important role in how well the model behaves. When using HMM, you should try to have some statistical knowledge of the data beforehead so that you can use initial $A$, $B$, and $\\pi$ parameters that represent well your system. Moreover, for really long sequences the code does not work at all! This is because we are multiplying lots of probabilities which in themselves are already equal or smaller than one! So you will usually encounter [underflow](https://stackoverflow.com/questions/34930570/at-what-point-should-i-worry-about-underflow-in-numpy-values) problems unless you use multiplicative constants in your code that take this into consideration (which I did not). This problem is stated in literature (see [2])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id=\"sec6\"></a> To Finish Off\n",
    "\n",
    "Hidden Markov models (HMMs) are a way of relating a sequence of observations to a sequence of hidden classes or hidden states that explain the observations.\n",
    "\n",
    "The process of discovering the sequence of hidden states, given the sequence of observations, is known as decoding or inference. The _Viterbi_ algorithm is commonly used for decoding.\n",
    "\n",
    "The parameters of an HMM are the $A$ transition probability matrix and the $B$ observation likelihood matrix. Both can be trained with the _Baum-Welch_ or forward-backward algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id=\"sec7\"></a> Going Further\n",
    "\n",
    "In our examples, we only used states with discrete observations. The quantity of ice-creams was always either 1, 2 or 3. But what if this quantity was actually continuous? Like the total volume of ice-cream eaten? For this there are HMM that use [Gaussian mixtures](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) (modelling their observations as random variables following normal distributions). These have many applications, and are widely used by Hedge Funds, e.g. to model and predict stock market regimes [3]. Moreover, these Gaussian HMM were initially (and are still) used in speech recognition software [2].\n",
    "\n",
    "If you enjoyed this topic, feel free to check the papers below (they were also provided in the .zip file) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id=\"sec8\"></a> References\n",
    "1. [Daniel Jurafsky & James H. Martin, Speech and Language Processing, Chapter A, Draft of September 11, 2018.](https://web.stanford.edu/~jurafsky/slp3/A.pdf)\n",
    "2. [Lawrence R. Rabiner, A tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, No 2, February 1989.](https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf)\n",
    "3. [Nguyet Nguyen, Hidden Markov Model for Stock Trading, International Journal of Financial Studies, 26 March 2018.](https://www.mdpi.com/2227-7072/6/2/36/pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
