{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirFrance Hackathon\n",
    "\n",
    "### Gabriel A. Moreira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df_train = pd.read_csv('/home/gabriel/Desktop/hackathonAirFrance-master/data/train_data.csv', sep=',')\n",
    "df_test = pd.read_csv('/home/gabriel/Desktop/hackathonAirFrance-master/data/test_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.84986045 0.12830408 0.01633434 0.00319357]\n"
     ]
    }
   ],
   "source": [
    "df = df_train\n",
    "for i in range(0,len(list(df))):\n",
    "    if i > 1 and i < 26:    \n",
    "        x = df.iloc[:,i]\n",
    "        df.iloc[:,i] = (x-min(x))/(max(x)-min(x))\n",
    "\n",
    "dfpca = pd.DataFrame()\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(df[['op_setting_1','op_setting_2','op_setting_3','sensor_1','sensor_2','sensor_3','sensor_4','sensor_5','sensor_6','sensor_7','sensor_8','sensor_9','sensor_10','sensor_11','sensor_12','sensor_13','sensor_14','sensor_15','sensor_16','sensor_17','sensor_18','sensor_19','sensor_20','sensor_21']].values)\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "dfpca['engine_no'] = df['engine_no']\n",
    "dfpca['pca-one'] = pca_result[:,0]\n",
    "dfpca['pca-two'] = pca_result[:,1] \n",
    "dfpca['pca-three'] = pca_result[:,2]\n",
    "dfpca['pca-four'] = pca_result[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707\n"
     ]
    }
   ],
   "source": [
    "df = dfpca\n",
    "N_ENGINES = min(max(df_test['engine_no'].tolist())+1, max(df_train['engine_no'].tolist())+1)\n",
    "SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df):\n",
    "    new_df = df\n",
    "    for i in range(0,len(list(df))):\n",
    "        if i > 1 and i < 26:\n",
    "            x = df.iloc[:,i]\n",
    "            df.iloc[:,i] = (x-min(x))/(max(x)-min(x))\n",
    "    new_df.drop(df.columns[26:33], axis=1, inplace=True)\n",
    "    new_df.drop(df.columns[0:3], axis=1, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def get_engine_data(df, engine, test=False, normalized=True):\n",
    "    df = df.copy(deep=True)\n",
    "    df = normalize_df(df)\n",
    "    \n",
    "    if not test:\n",
    "        X_data = df.loc[df['engine_no'] == engine].iloc[:,2:-1]\n",
    "        Y_data = df.loc[df['engine_no'] == engine].iloc[:,-1]\n",
    "        return [X_data.reset_index(drop=True), Y_data.reset_index(drop=True)]\n",
    "    else:\n",
    "        X_data = df.loc[df['engine_no'] == engine].iloc[:,2:]\n",
    "        return X_data.reset_index(drop=True)\n",
    "      \n",
    "def generate_train_dataset(df):\n",
    "    x = []\n",
    "    y = []\n",
    "    for engine in range(N_ENGINES):\n",
    "        rule = [True, False]\n",
    "        slice_to_fail = random.choice(rule)\n",
    "        total_length = df.loc[df.iloc[:,0] == engine].shape[0]\n",
    "        if total_length > SEQUENCE_LENGTH:\n",
    "            if slice_to_fail: \n",
    "                for i in range(50):\n",
    "                    if total_length > 100 + SEQUENCE_LENGTH:\n",
    "                        seq_df = df.loc[df['engine_no'] == engine]\n",
    "                        seq_df.reset_index(drop=True)\n",
    "                        total_length = seq_df.shape[0]\n",
    "                        seq_df = seq_df[total_length-100-SEQUENCE_LENGTH+i:total_length-100+i]\n",
    "                        seq_df = seq_df.reset_index(drop=True)\n",
    "                        seq_df = seq_df.iloc[:,1:]\n",
    "                        x.append(seq_df)\n",
    "                        y.append(1)\n",
    "\n",
    "                if total_length < 100 + SEQUENCE_LENGTH:\n",
    "                    seq_df = df.loc[df['engine_no'] == engine]\n",
    "                    seq_df.reset_index(drop=True)\n",
    "                    total_length = seq_df.shape[0]\n",
    "                    seq_df = seq_df[total_length-SEQUENCE_LENGTH:total_length]\n",
    "                    seq_df = seq_df.reset_index(drop=True)\n",
    "                    seq_df = seq_df.iloc[:,1:] \n",
    "                    x.append(seq_df)\n",
    "                    y.append(1)\n",
    "            else:\n",
    "                for i in range(total_length-100-SEQUENCE_LENGTH):\n",
    "                    if total_length > 100 + SEQUENCE_LENGTH:\n",
    "                        seq_df = df.loc[df['engine_no'] == engine]\n",
    "                        seq_df.reset_index(drop=True, inplace=True)\n",
    "                        seq_df = seq_df[0+i:SEQUENCE_LENGTH+i]\n",
    "                        seq_df = seq_df.reset_index(drop=True)\n",
    "                        seq_df = seq_df.iloc[:,1:] \n",
    "                        x.append(seq_df)\n",
    "                        y.append(0)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generate_train_dataset(df)\n",
    "x = a[0]\n",
    "y = a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POINTS = 18000\n",
    "mask = np.arange(0,len(x), len(x)/N_POINTS, dtype=int)\n",
    "\n",
    "x = list(x[i] for i in mask)\n",
    "y = list(y[i] for i in mask)\n",
    "\n",
    "train_indices = random.sample(range(len(x)), int(0.7*len(x)))\n",
    "test_indices = [i for i in range(len(train_indices)) if i not in train_indices]\n",
    "\n",
    "trainX = np.empty((len(train_indices), SEQUENCE_LENGTH, 4))\n",
    "testX = np.empty((len(x) - len(train_indices), SEQUENCE_LENGTH, 4))\n",
    "trainy = np.empty((len(train_indices), 1))\n",
    "testy = np.empty((len(x) - len(train_indices), 1))\n",
    "\n",
    "ii = sorted(train_indices)\n",
    "    \n",
    "for i in range(len(train_indices)):\n",
    "    trainX[i,:,:] = x[ii[i]].values\n",
    "    trainy[i,:] = y[ii[i]]\n",
    "\n",
    "for i in range(len(test_indices)):\n",
    "    testX[i,:,:] = x[ii[i]].values\n",
    "    testy[i,:] = y[ii[i]]\n",
    "    \n",
    "from keras.utils import to_categorical\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12600, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Neural Network - LSTM implementation with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def fit_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 15, 128\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], 2\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "    opt = Adam(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    return model\n",
    " \n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    " \n",
    "def run_training(trainX, trainy, testX, testy, repeats=2):\n",
    "    batch_size = 128\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        model = fit_model(trainX, trainy, testX, testy)\n",
    "        _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "        score = accuracy * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "12600/12600 [==============================] - 3s 251us/step - loss: 0.6294 - acc: 0.6863\n",
      "Epoch 2/15\n",
      "12600/12600 [==============================] - 3s 205us/step - loss: 0.6210 - acc: 0.6878\n",
      "Epoch 3/15\n",
      "12600/12600 [==============================] - 3s 208us/step - loss: 0.6179 - acc: 0.6891\n",
      "Epoch 4/15\n",
      "12600/12600 [==============================] - 3s 208us/step - loss: 0.6148 - acc: 0.6956\n",
      "Epoch 5/15\n",
      "12600/12600 [==============================] - 3s 253us/step - loss: 0.6089 - acc: 0.6998\n",
      "Epoch 6/15\n",
      "12600/12600 [==============================] - 3s 229us/step - loss: 0.6074 - acc: 0.7033\n",
      "Epoch 7/15\n",
      "12600/12600 [==============================] - 3s 213us/step - loss: 0.6061 - acc: 0.7043\n",
      "Epoch 8/15\n",
      "12600/12600 [==============================] - 3s 228us/step - loss: 0.6042 - acc: 0.7047\n",
      "Epoch 9/15\n",
      "12600/12600 [==============================] - 3s 221us/step - loss: 0.6054 - acc: 0.7049\n",
      "Epoch 10/15\n",
      "12600/12600 [==============================] - 3s 224us/step - loss: 0.6044 - acc: 0.7066\n",
      "Epoch 11/15\n",
      "12600/12600 [==============================] - 3s 230us/step - loss: 0.6020 - acc: 0.7061\n",
      "Epoch 12/15\n",
      "12600/12600 [==============================] - 4s 312us/step - loss: 0.6018 - acc: 0.7066\n",
      "Epoch 13/15\n",
      "12600/12600 [==============================] - 3s 260us/step - loss: 0.6030 - acc: 0.7060\n",
      "Epoch 14/15\n",
      "12600/12600 [==============================] - 3s 244us/step - loss: 0.6019 - acc: 0.7055\n",
      "Epoch 15/15\n",
      "12600/12600 [==============================] - 4s 285us/step - loss: 0.5983 - acc: 0.7101\n",
      "5400/5400 [==============================] - 1s 109us/step\n",
      ">#1: 63.148\n",
      "Epoch 1/15\n",
      "12600/12600 [==============================] - 3s 266us/step - loss: 0.6282 - acc: 0.6830\n",
      "Epoch 2/15\n",
      "12600/12600 [==============================] - 3s 230us/step - loss: 0.6187 - acc: 0.6907\n",
      "Epoch 3/15\n",
      "12600/12600 [==============================] - 3s 230us/step - loss: 0.6130 - acc: 0.6963\n",
      "Epoch 4/15\n",
      "12600/12600 [==============================] - 3s 275us/step - loss: 0.6094 - acc: 0.6997\n",
      "Epoch 5/15\n",
      "12600/12600 [==============================] - 3s 227us/step - loss: 0.6103 - acc: 0.6976\n",
      "Epoch 6/15\n",
      "12600/12600 [==============================] - 4s 281us/step - loss: 0.6084 - acc: 0.7000\n",
      "Epoch 7/15\n",
      "12600/12600 [==============================] - 3s 274us/step - loss: 0.6085 - acc: 0.6996\n",
      "Epoch 8/15\n",
      "12600/12600 [==============================] - 3s 231us/step - loss: 0.6065 - acc: 0.7021\n",
      "Epoch 9/15\n",
      "12600/12600 [==============================] - 3s 255us/step - loss: 0.6057 - acc: 0.7044\n",
      "Epoch 10/15\n",
      "12600/12600 [==============================] - 3s 226us/step - loss: 0.6042 - acc: 0.7060\n",
      "Epoch 11/15\n",
      "12600/12600 [==============================] - 3s 241us/step - loss: 0.6046 - acc: 0.7045\n",
      "Epoch 12/15\n",
      "12600/12600 [==============================] - 3s 251us/step - loss: 0.6021 - acc: 0.7055\n",
      "Epoch 13/15\n",
      "12600/12600 [==============================] - 3s 245us/step - loss: 0.6012 - acc: 0.7074\n",
      "Epoch 14/15\n",
      "12600/12600 [==============================] - 3s 276us/step - loss: 0.6009 - acc: 0.7068\n",
      "Epoch 15/15\n",
      "12600/12600 [==============================] - 3s 252us/step - loss: 0.5985 - acc: 0.7102\n",
      "5400/5400 [==============================] - 1s 120us/step\n",
      ">#2: 70.389\n",
      "[63.14814814594057, 70.38888888005857]\n",
      "Accuracy: 66.769% (+/-3.620)\n"
     ]
    }
   ],
   "source": [
    "model = run_training(trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.85073548 0.12828674 0.01769858 0.00151942]\n",
      "(20, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df_test\n",
    "\n",
    "for i in range(0,len(list(df))):  \n",
    "    if i > 1 and i < 26:      \n",
    "        x = df.iloc[:,i]\n",
    "        df.iloc[:,i] = (x-min(x))/(max(x)-min(x))\n",
    "\n",
    "dfpca = pd.DataFrame()\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(df[['op_setting_1', 'op_setting_2',\n",
    "                                   'op_setting_3', 'sensor_1',\n",
    "                                   'sensor_2', 'sensor_3',\n",
    "                                   'sensor_4', 'sensor_5',\n",
    "                                   'sensor_6','sensor_7',\n",
    "                                   'sensor_8','sensor_9',\n",
    "                                   'sensor_10','sensor_11',\n",
    "                                   'sensor_12','sensor_13',\n",
    "                                   'sensor_14','sensor_15',\n",
    "                                   'sensor_16','sensor_17',\n",
    "                                   'sensor_18','sensor_19',\n",
    "                                   'sensor_20','sensor_21']].values)\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "dfpca['engine_no'] = df['engine_no']\n",
    "dfpca['pca-one'] = pca_result[:,0]\n",
    "dfpca['pca-two'] = pca_result[:,1] \n",
    "dfpca['pca-three'] = pca_result[:,2]\n",
    "dfpca['pca-four'] = pca_result[:,3]\n",
    "\n",
    "df = dfpca\n",
    "\n",
    "def _generate_test_dataset(df):\n",
    "    engine_data = []\n",
    "    engine_number = []\n",
    "    for engine in range(N_ENGINES):\n",
    "        seq_df = df.loc[df.iloc[:,0] == engine]\n",
    "        seq_df.reset_index(drop=True, inplace=True)\n",
    "        total_length = seq_df.shape[0]\n",
    "        if total_length > SEQUENCE_LENGTH:\n",
    "            seq_df = seq_df[total_length-SEQUENCE_LENGTH:]\n",
    "            seq_df.reset_index(drop=True, inplace=True)\n",
    "            seq_df = seq_df.iloc[:,1:]\n",
    "            engine_number.append(engine)\n",
    "            engine_data.append(seq_df)\n",
    "    return [engine_number, engine_data]\n",
    "\n",
    "a = _generate_test_dataset(df)\n",
    "\n",
    "eng_numb = a[0]\n",
    "eng_data = a[1]\n",
    "\n",
    "print(eng_data[0].values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08432797 0.91567206]]\n",
      "[[0.22428171 0.7757183 ]]\n",
      "[[0.6735742  0.32642573]]\n",
      "[[0.7318171  0.26818287]]\n",
      "[[0.18222284 0.8177771 ]]\n",
      "[[0.6934476 0.3065524]]\n",
      "[[0.69619244 0.30380756]]\n",
      "[[0.24522673 0.75477326]]\n",
      "[[0.03666515 0.96333486]]\n",
      "[[0.6720598  0.32794023]]\n",
      "[[0.6730449  0.32695502]]\n",
      "[[0.09804195 0.901958  ]]\n",
      "[[0.68361866 0.3163813 ]]\n",
      "[[0.6852326  0.31476742]]\n",
      "[[0.710335 0.289665]]\n",
      "[[0.6718576  0.32814243]]\n",
      "[[0.69679284 0.3032072 ]]\n",
      "[[0.68023187 0.3197681 ]]\n",
      "[[0.6726025  0.32739747]]\n",
      "[[0.6787678 0.3212322]]\n",
      "[[0.6761888  0.32381117]]\n",
      "[[0.6888351  0.31116495]]\n",
      "[[0.31964985 0.6803502 ]]\n",
      "[[0.69636524 0.30363473]]\n",
      "[[0.697402   0.30259803]]\n",
      "[[0.6713407 0.3286593]]\n",
      "[[0.67201406 0.32798597]]\n",
      "[[0.71564484 0.28435516]]\n",
      "[[0.69285625 0.30714375]]\n",
      "[[0.68130857 0.31869146]]\n",
      "[[0.3000998  0.69990015]]\n",
      "[[0.6735033  0.32649672]]\n",
      "[[0.6719555  0.32804453]]\n",
      "[[0.6936692  0.30633086]]\n",
      "[[0.66474676 0.33525327]]\n",
      "[[0.71332675 0.2866732 ]]\n",
      "[[0.6788 0.3212]]\n",
      "[[0.672197  0.3278031]]\n",
      "[[0.72072566 0.2792744 ]]\n",
      "[[0.6922652  0.30773482]]\n",
      "[[0.72249573 0.2775043 ]]\n",
      "[[0.73682564 0.26317438]]\n",
      "[[0.67204684 0.32795322]]\n",
      "[[0.6940906  0.30590943]]\n",
      "[[0.6728893 0.3271107]]\n",
      "[[0.7088946  0.29110542]]\n",
      "[[0.6481085  0.35189158]]\n",
      "[[0.2807722 0.7192278]]\n",
      "[[0.6728627  0.32713732]]\n",
      "[[0.24358189 0.7564181 ]]\n",
      "[[0.6980824 0.3019176]]\n",
      "[[0.6878045  0.31219545]]\n",
      "[[0.67198485 0.32801518]]\n",
      "[[0.67156553 0.32843444]]\n",
      "[[0.3973015  0.60269856]]\n",
      "[[0.10025026 0.89974976]]\n",
      "[[0.7009861  0.29901394]]\n",
      "[[0.5553616  0.44463837]]\n",
      "[[0.21086636 0.7891337 ]]\n",
      "[[0.71018857 0.28981143]]\n",
      "[[0.69373524 0.30626476]]\n",
      "[[0.67299676 0.3270032 ]]\n",
      "[[0.6727474  0.32725263]]\n",
      "[[0.6723096  0.32769045]]\n",
      "[[0.67178935 0.3282107 ]]\n",
      "[[0.6803115  0.31968856]]\n",
      "[[0.7038827 0.2961173]]\n",
      "[[0.7087265 0.2912735]]\n",
      "[[0.6719623  0.32803768]]\n",
      "[[0.12453607 0.87546396]]\n",
      "[[0.6727969  0.32720315]]\n",
      "[[0.6900936 0.3099064]]\n",
      "[[0.6831715  0.31682858]]\n",
      "[[0.6727846  0.32721537]]\n",
      "[[0.74352694 0.256473  ]]\n",
      "[[0.694392   0.30560794]]\n",
      "[[0.6835063 0.3164937]]\n",
      "[[0.6908262  0.30917385]]\n",
      "[[0.671406   0.32859403]]\n",
      "[[0.6712271  0.32877293]]\n",
      "[[0.6731176 0.3268824]]\n",
      "[[0.46034124 0.5396588 ]]\n",
      "[[0.67301756 0.32698247]]\n",
      "[[0.19157311 0.8084269 ]]\n",
      "[[0.6815576 0.3184424]]\n",
      "[[0.72102404 0.2789759 ]]\n",
      "[[0.7114466 0.2885535]]\n",
      "[[0.63717026 0.36282977]]\n",
      "[[0.70321125 0.2967887 ]]\n",
      "[[0.43089804 0.569102  ]]\n",
      "[[0.14631511 0.85368484]]\n",
      "[[0.2825763 0.7174236]]\n",
      "[[0.13502951 0.86497045]]\n",
      "[[0.2604535  0.73954654]]\n",
      "[[0.7194564  0.28054368]]\n",
      "[[0.67229897 0.32770106]]\n",
      "[[0.678891 0.321109]]\n",
      "[[0.0358986  0.96410143]]\n",
      "[[0.67143464 0.32856542]]\n",
      "[[0.67268014 0.32731986]]\n",
      "[[0.02543228 0.9745677 ]]\n",
      "[[0.67322063 0.3267794 ]]\n",
      "[[0.25189072 0.7481093 ]]\n",
      "[[0.298829 0.701171]]\n",
      "[[0.26464036 0.73535967]]\n",
      "[[0.20103817 0.7989618 ]]\n",
      "[[0.6892986  0.31070137]]\n",
      "[[0.67821294 0.3217871 ]]\n",
      "[[0.6729782 0.3270218]]\n",
      "[[0.67313087 0.32686913]]\n",
      "[[0.706848   0.29315197]]\n",
      "[[0.6830357 0.3169643]]\n",
      "[[0.66228306 0.3377169 ]]\n",
      "[[0.6943883  0.30561176]]\n",
      "[[0.6714869  0.32851306]]\n",
      "[[0.67483974 0.3251603 ]]\n",
      "[[0.6721297 0.3278703]]\n",
      "[[0.7137335  0.28626657]]\n",
      "[[0.67245924 0.32754076]]\n",
      "[[0.6082271  0.39177287]]\n",
      "[[0.67184854 0.32815152]]\n",
      "[[0.6896776  0.31032237]]\n",
      "[[0.40464687 0.5953531 ]]\n",
      "[[0.6938645  0.30613545]]\n",
      "[[0.676995   0.32300502]]\n",
      "[[0.7253317  0.27466828]]\n",
      "[[0.6915883  0.30841178]]\n",
      "[[0.67181426 0.32818577]]\n",
      "[[0.13401045 0.86598957]]\n",
      "[[0.67272145 0.32727852]]\n",
      "[[0.68907344 0.31092653]]\n",
      "[[0.7218421 0.2781579]]\n",
      "[[0.6912558  0.30874416]]\n",
      "[[0.6826883  0.31731173]]\n",
      "[[0.6719236  0.32807642]]\n",
      "[[0.17450838 0.8254916 ]]\n",
      "[[0.671367   0.32863295]]\n",
      "[[0.06300824 0.93699175]]\n",
      "[[0.21076572 0.7892343 ]]\n",
      "[[0.22841288 0.77158713]]\n",
      "[[0.20526734 0.79473263]]\n",
      "[[0.6734726  0.32652745]]\n",
      "[[0.01498921 0.9850108 ]]\n",
      "[[0.34132722 0.65867275]]\n",
      "[[0.7272114  0.27278855]]\n",
      "[[0.67193735 0.3280627 ]]\n",
      "[[0.22101596 0.778984  ]]\n",
      "[[0.70852363 0.29147637]]\n",
      "[[0.67183703 0.32816297]]\n",
      "[[0.7161735 0.2838264]]\n",
      "[[0.3164868 0.6835132]]\n",
      "[[0.6983938  0.30160618]]\n",
      "[[0.6929599  0.30704015]]\n",
      "[[0.7118251  0.28817493]]\n",
      "[[0.7105205  0.28947958]]\n",
      "[[0.7107515  0.28924856]]\n",
      "[[0.05463373 0.9453662 ]]\n",
      "[[0.19476154 0.8052385 ]]\n",
      "[[0.7048556  0.29514438]]\n",
      "[[0.67238116 0.32761884]]\n",
      "[[0.6726918  0.32730818]]\n",
      "[[0.67226195 0.32773805]]\n",
      "[[0.70453894 0.29546106]]\n",
      "[[0.69423586 0.30576414]]\n",
      "[[0.6838793  0.31612068]]\n",
      "[[0.13596956 0.8640304 ]]\n",
      "[[0.28627354 0.7137264 ]]\n",
      "[[0.70925766 0.29074237]]\n",
      "[[0.6729863  0.32701373]]\n",
      "[[0.67265016 0.32734987]]\n",
      "[[0.6727903  0.32720965]]\n",
      "[[0.68583417 0.31416586]]\n",
      "[[0.6723402  0.32765976]]\n",
      "[[0.69605935 0.3039406 ]]\n",
      "[[0.69364846 0.30635148]]\n",
      "[[0.0540934 0.9459066]]\n",
      "[[0.15594935 0.8440507 ]]\n",
      "[[0.6741536  0.32584643]]\n",
      "[[0.70164734 0.29835266]]\n",
      "[[0.7049376  0.29506242]]\n",
      "[[0.38592112 0.6140789 ]]\n",
      "[[0.7029556  0.29704437]]\n",
      "[[0.4366196 0.5633804]]\n",
      "[[0.13815951 0.8618405 ]]\n",
      "[[0.6721053  0.32789475]]\n",
      "[[0.7014976 0.2985024]]\n",
      "[[0.7231642  0.27683574]]\n",
      "[[0.2999653 0.7000347]]\n",
      "[[0.71946734 0.2805326 ]]\n",
      "[[0.67333955 0.32666048]]\n",
      "[[0.44565797 0.55434203]]\n",
      "[[0.67189527 0.32810476]]\n",
      "[[0.10820355 0.8917965 ]]\n",
      "[[0.69078577 0.3092142 ]]\n",
      "[[0.69599336 0.30400664]]\n",
      "[[0.6729346 0.3270654]]\n",
      "[[0.6715945 0.3284055]]\n",
      "[[0.716768   0.28323197]]\n",
      "[[0.65059906 0.34940103]]\n",
      "[[0.6715086  0.32849133]]\n",
      "[[0.67177397 0.32822603]]\n",
      "[[0.6717774  0.32822257]]\n",
      "[[0.6734296  0.32657042]]\n",
      "[[0.15334424 0.8466558 ]]\n",
      "[[0.67282665 0.32717335]]\n",
      "[[0.01204561 0.9879544 ]]\n",
      "[[0.11335614 0.8866438 ]]\n",
      "[[0.6712146  0.32878545]]\n",
      "[[0.6722263  0.32777366]]\n",
      "[[0.38185778 0.61814225]]\n",
      "[[0.7056997  0.29430026]]\n",
      "[[0.68578404 0.31421596]]\n",
      "[[0.7153937 0.2846063]]\n",
      "[[0.64232785 0.35767218]]\n",
      "[[0.43785417 0.56214577]]\n",
      "[[0.67060375 0.32939622]]\n",
      "[[0.67300004 0.32700002]]\n",
      "[[0.706545   0.29345503]]\n",
      "[[0.69772255 0.30227742]]\n",
      "[[0.25351423 0.74648577]]\n",
      "[[0.6730343 0.3269657]]\n",
      "[[0.67296153 0.32703853]]\n",
      "[[0.69463456 0.30536553]]\n",
      "[[0.70907784 0.29092214]]\n",
      "[[0.10569823 0.8943018 ]]\n",
      "[[0.71053565 0.28946438]]\n",
      "[[0.01060033 0.9893996 ]]\n",
      "[[0.67223287 0.3277671 ]]\n",
      "[[0.61027455 0.38972548]]\n",
      "[[0.3327137 0.6672863]]\n",
      "[[0.6714745  0.32852554]]\n",
      "[[0.7003525  0.29964754]]\n",
      "[[0.6952057  0.30479422]]\n",
      "[[0.6722535 0.3277465]]\n",
      "[[0.6722361  0.32776392]]\n",
      "[[0.31956804 0.680432  ]]\n",
      "[[0.6733507  0.32664934]]\n",
      "[[0.18747948 0.8125205 ]]\n",
      "[[0.6709495  0.32905045]]\n",
      "[[0.18087612 0.81912386]]\n",
      "[[0.20196816 0.7980318 ]]\n",
      "[[0.6821185  0.31788152]]\n",
      "[[0.72771233 0.27228764]]\n",
      "[[0.6571525  0.34284747]]\n",
      "[[0.67246616 0.3275338 ]]\n",
      "[[0.7187053  0.28129476]]\n",
      "[[0.6715141 0.3284859]]\n",
      "[[0.71592367 0.2840763 ]]\n",
      "[[0.71322566 0.28677434]]\n",
      "[[0.67213213 0.32786787]]\n",
      "[[0.68644327 0.31355673]]\n",
      "[[0.7182842  0.28171587]]\n",
      "[[0.6716386  0.32836136]]\n",
      "[[0.26058638 0.7394137 ]]\n",
      "[[0.6869582 0.3130418]]\n",
      "[[0.6995711  0.30042893]]\n",
      "[[0.6711015 0.3288985]]\n",
      "[[0.6717001 0.3282999]]\n",
      "[[0.671392 0.328608]]\n",
      "[[0.327962   0.67203796]]\n",
      "[[0.69773346 0.30226657]]\n",
      "[[0.24145974 0.7585403 ]]\n",
      "[[0.3176148 0.6823852]]\n",
      "[[0.67557013 0.32442993]]\n",
      "[[0.6734835  0.32651654]]\n",
      "[[0.7088426  0.29115745]]\n",
      "[[0.6938413  0.30615866]]\n",
      "[[0.6964248 0.3035752]]\n",
      "[[0.6712673 0.3287328]]\n",
      "[[0.6715372 0.3284628]]\n",
      "[[0.68260264 0.31739727]]\n",
      "[[0.6819332  0.31806675]]\n",
      "[[0.67178494 0.32821503]]\n",
      "[[0.68678355 0.31321642]]\n",
      "[[0.71656793 0.28343204]]\n",
      "[[0.6735684  0.32643157]]\n",
      "[[0.67222226 0.3277777 ]]\n",
      "[[0.6600671  0.33993286]]\n",
      "[[0.7015984  0.29840162]]\n",
      "[[0.48868078 0.5113193 ]]\n",
      "[[0.6719985  0.32800147]]\n",
      "[[0.07427292 0.92572707]]\n",
      "[[0.7049692  0.29503077]]\n",
      "[[0.14614847 0.85385156]]\n",
      "[[0.08186757 0.9181324 ]]\n",
      "[[0.6910237  0.30897635]]\n",
      "[[0.6995428  0.30045727]]\n",
      "[[0.16301438 0.8369856 ]]\n",
      "[[0.67212766 0.32787234]]\n",
      "[[0.6717508  0.32824925]]\n",
      "[[0.67203516 0.32796487]]\n",
      "[[0.6950296  0.30497035]]\n",
      "[[0.70085555 0.29914448]]\n",
      "[[0.4889124  0.51108754]]\n",
      "[[0.09367134 0.9063286 ]]\n",
      "[[0.73169196 0.2683081 ]]\n",
      "[[0.672454 0.327546]]\n",
      "[[0.67275256 0.3272474 ]]\n",
      "[[0.67105097 0.32894906]]\n",
      "[[0.42263177 0.57736826]]\n",
      "[[0.6831776  0.31682244]]\n",
      "[[0.7100638  0.28993618]]\n",
      "[[0.7033354 0.2966646]]\n",
      "[[0.6795534  0.32044664]]\n",
      "[[0.41981965 0.58018035]]\n",
      "[[0.7064179  0.29358214]]\n",
      "[[0.7139763 0.2860237]]\n",
      "[[0.14773434 0.85226566]]\n",
      "[[0.67069167 0.32930833]]\n",
      "[[0.6724323  0.32756764]]\n",
      "[[0.17742813 0.8225719 ]]\n",
      "[[0.67893344 0.32106665]]\n",
      "[[0.6728842  0.32711577]]\n",
      "[[0.6954593 0.3045407]]\n",
      "[[0.67184484 0.3281552 ]]\n",
      "[[0.67714995 0.3228501 ]]\n",
      "[[0.08228777 0.9177122 ]]\n",
      "[[0.09227919 0.90772086]]\n",
      "[[0.6818655  0.31813446]]\n",
      "[[0.6716952 0.3283049]]\n",
      "[[0.30433798 0.69566196]]\n",
      "[[0.67219913 0.32780093]]\n",
      "[[0.68905026 0.3109497 ]]\n",
      "[[0.17274791 0.82725215]]\n",
      "[[0.2881679 0.7118321]]\n",
      "[[0.67228085 0.32771918]]\n",
      "[[0.6366548  0.36334518]]\n",
      "[[0.68103397 0.318966  ]]\n",
      "[[0.68000513 0.31999487]]\n",
      "[[0.6729707  0.32702932]]\n",
      "[[0.69555354 0.30444643]]\n",
      "[[0.07984892 0.9201511 ]]\n",
      "[[0.698007   0.30199304]]\n",
      "[[0.70823574 0.29176426]]\n",
      "[[0.68853664 0.31146333]]\n",
      "[[0.6200359 0.3799641]]\n",
      "[[0.41418794 0.5858121 ]]\n",
      "[[0.6917735 0.3082265]]\n",
      "[[0.5934909 0.4065091]]\n",
      "[[0.7101258  0.28987426]]\n",
      "[[0.7047367  0.29526323]]\n",
      "[[0.6723842 0.3276158]]\n",
      "[[0.671663 0.328337]]\n",
      "[[0.67372054 0.32627952]]\n",
      "[[0.672494 0.327506]]\n",
      "[[0.6973672  0.30263284]]\n",
      "[[0.6727233 0.3272767]]\n",
      "[[0.671354   0.32864594]]\n",
      "[[0.6720278  0.32797217]]\n",
      "[[0.17888223 0.82111776]]\n",
      "[[0.69183344 0.3081665 ]]\n",
      "[[0.6711303 0.3288697]]\n",
      "[[0.6883506  0.31164944]]\n",
      "[[0.672082   0.32791796]]\n",
      "[[0.12310304 0.8768969 ]]\n",
      "[[0.72501504 0.27498502]]\n",
      "[[0.67137825 0.32862177]]\n",
      "[[0.66364604 0.33635396]]\n",
      "[[0.12351391 0.87648606]]\n",
      "[[0.6639074  0.33609256]]\n",
      "[[0.70005214 0.29994783]]\n",
      "[[0.67156714 0.3284329 ]]\n",
      "[[0.71516967 0.2848303 ]]\n",
      "[[0.6788107  0.32118922]]\n",
      "[[0.6908322  0.30916777]]\n",
      "[[0.6719634  0.32803667]]\n",
      "[[0.13147978 0.86852026]]\n",
      "[[0.6995344  0.30046558]]\n",
      "[[0.6891766  0.31082335]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59669936 0.40330064]]\n",
      "[[0.66096425 0.33903578]]\n",
      "[[0.6726674  0.32733265]]\n",
      "[[0.69378275 0.3062172 ]]\n",
      "[[0.6716197  0.32838026]]\n",
      "[[0.71312493 0.28687504]]\n",
      "[[0.7043822  0.29561782]]\n",
      "[[0.7319289 0.2680711]]\n",
      "[[0.5684682 0.4315318]]\n",
      "[[0.67851454 0.32148546]]\n",
      "[[0.67246497 0.32753497]]\n",
      "[[0.6928317  0.30716833]]\n",
      "[[0.6724955 0.3275045]]\n",
      "[[0.6728871 0.3271129]]\n",
      "[[0.1626742 0.8373258]]\n",
      "[[0.6906729  0.30932713]]\n",
      "[[0.67818296 0.32181698]]\n",
      "[[0.41570598 0.584294  ]]\n",
      "[[0.6724936  0.32750645]]\n",
      "[[0.67228866 0.32771134]]\n",
      "[[0.14318158 0.85681844]]\n",
      "[[0.6813407  0.31865928]]\n",
      "[[0.6720577 0.3279423]]\n",
      "[[0.70408136 0.29591867]]\n",
      "[[0.6835637  0.31643623]]\n",
      "[[0.696584   0.30341595]]\n",
      "[[0.6878845 0.3121155]]\n",
      "[[0.6796394  0.32036063]]\n",
      "[[0.6855701  0.31442985]]\n",
      "[[0.67240655 0.32759348]]\n",
      "[[0.68199486 0.3180051 ]]\n",
      "[[0.03253253 0.9674675 ]]\n",
      "[[0.72085315 0.2791468 ]]\n",
      "[[0.6349909  0.36500916]]\n",
      "[[0.71405375 0.2859462 ]]\n",
      "[[0.6735083  0.32649174]]\n",
      "[[0.67258406 0.32741594]]\n",
      "[[0.18019871 0.81980133]]\n",
      "[[0.2685419 0.7314581]]\n",
      "[[0.67065936 0.32934067]]\n",
      "[[0.69266886 0.3073311 ]]\n",
      "[[0.71798146 0.28201854]]\n",
      "[[0.6899078  0.31009224]]\n",
      "[[0.7249726 0.2750274]]\n",
      "[[0.03949151 0.9605085 ]]\n",
      "[[0.6939853  0.30601472]]\n",
      "[[0.69169927 0.3083007 ]]\n",
      "[[0.44571754 0.5542825 ]]\n",
      "[[0.2920642 0.7079358]]\n",
      "[[0.6716766  0.32832342]]\n",
      "[[0.7250885 0.2749115]]\n",
      "[[0.68314654 0.3168534 ]]\n",
      "[[0.67352116 0.32647878]]\n",
      "[[0.69060194 0.30939808]]\n",
      "[[0.67369676 0.3263032 ]]\n",
      "[[0.06292538 0.93707466]]\n",
      "[[0.6728295  0.32717052]]\n",
      "[[0.71779466 0.28220534]]\n",
      "[[0.67382187 0.32617816]]\n",
      "[[0.67158085 0.32841918]]\n",
      "[[0.69820005 0.30179995]]\n",
      "[[0.68361515 0.31638482]]\n",
      "[[0.70066524 0.29933473]]\n",
      "[[0.67201704 0.32798293]]\n",
      "[[0.08920121 0.9107988 ]]\n",
      "[[0.71266603 0.28733402]]\n",
      "[[0.6728779 0.3271221]]\n",
      "[[0.6676515  0.33234856]]\n",
      "[[0.6669828 0.3330172]]\n",
      "[[0.6852932  0.31470683]]\n",
      "[[0.07560908 0.9243909 ]]\n",
      "[[0.5143552 0.4856448]]\n",
      "[[0.68384755 0.31615245]]\n",
      "[[0.70912224 0.29087785]]\n",
      "[[0.6723678 0.3276322]]\n",
      "[[0.6735064 0.3264936]]\n",
      "[[0.7024962  0.29750377]]\n",
      "[[0.3620301 0.6379699]]\n",
      "[[0.70982105 0.29017904]]\n",
      "[[0.70161504 0.29838496]]\n",
      "[[0.7062938  0.29370618]]\n",
      "[[0.69251865 0.30748135]]\n",
      "[[0.67246497 0.32753497]]\n",
      "[[0.6593302 0.3406698]]\n",
      "[[0.7022345  0.29776552]]\n",
      "[[0.67357093 0.32642907]]\n",
      "[[0.6732205  0.32677945]]\n",
      "[[0.67204005 0.32795995]]\n",
      "[[0.6944848  0.30551523]]\n",
      "[[0.68991345 0.31008652]]\n",
      "[[0.67299294 0.32700709]]\n",
      "[[0.1621023 0.8378977]]\n",
      "[[0.68838054 0.31161943]]\n",
      "[[0.6716138  0.32838622]]\n",
      "[[0.67090166 0.32909834]]\n",
      "[[0.49405682 0.5059432 ]]\n",
      "[[0.68286514 0.3171349 ]]\n",
      "[[0.70892364 0.29107642]]\n",
      "[[0.69093984 0.30906016]]\n",
      "[[0.65775985 0.34224015]]\n",
      "[[0.69849455 0.30150545]]\n",
      "[[0.6634825  0.33651748]]\n",
      "[[0.69077176 0.30922818]]\n",
      "[[0.6821821  0.31781793]]\n",
      "[[0.7061663  0.29383367]]\n",
      "[[0.69790536 0.30209467]]\n",
      "[[0.68777823 0.31222177]]\n",
      "[[0.6893119 0.3106881]]\n",
      "[[0.69235045 0.30764952]]\n",
      "[[0.6718182  0.32818183]]\n",
      "[[0.6842487  0.31575137]]\n",
      "[[0.7074296  0.29257038]]\n",
      "[[0.6843449  0.31565508]]\n",
      "[[0.02089758 0.97910243]]\n",
      "[[0.6917822  0.30821782]]\n",
      "[[0.67175037 0.32824966]]\n",
      "[[0.68443286 0.3155671 ]]\n",
      "[[0.6726705 0.3273295]]\n",
      "[[0.02343553 0.9765645 ]]\n",
      "[[0.14812838 0.8518716 ]]\n",
      "[[0.6833764  0.31662366]]\n",
      "[[0.67157936 0.3284206 ]]\n",
      "[[0.67223734 0.3277626 ]]\n",
      "[[0.67220044 0.32779953]]\n",
      "[[0.7040394 0.2959606]]\n",
      "[[0.72004443 0.27995554]]\n",
      "[[0.7052923  0.29470772]]\n",
      "[[0.6644483 0.3355517]]\n",
      "[[0.7326229  0.26737705]]\n",
      "[[0.209968 0.790032]]\n",
      "[[0.06520671 0.9347933 ]]\n",
      "[[0.7066184 0.2933816]]\n",
      "[[0.7079572  0.29204282]]\n",
      "[[0.17904557 0.8209544 ]]\n",
      "[[0.3712752  0.62872475]]\n",
      "[[0.6730012 0.3269987]]\n",
      "[[0.6776178 0.3223822]]\n",
      "[[0.0775643 0.9224357]]\n",
      "[[0.6721871  0.32781288]]\n",
      "[[0.6911557  0.30884433]]\n",
      "[[0.6974225 0.3025774]]\n",
      "[[0.7020009  0.29799902]]\n",
      "[[0.6992072  0.30079284]]\n",
      "[[0.1715013 0.8284988]]\n",
      "[[0.704453   0.29554698]]\n",
      "[[0.67155915 0.3284408 ]]\n",
      "[[0.67224133 0.3277587 ]]\n",
      "[[0.67373526 0.3262647 ]]\n",
      "[[0.67249095 0.327509  ]]\n",
      "[[0.67077345 0.32922652]]\n",
      "[[0.6714241 0.3285759]]\n",
      "[[0.71175885 0.28824118]]\n",
      "[[0.16313969 0.8368603 ]]\n",
      "[[0.69583553 0.30416444]]\n",
      "[[0.67133886 0.32866114]]\n",
      "[[0.69412196 0.3058781 ]]\n",
      "[[0.14209622 0.8579038 ]]\n",
      "[[0.67222065 0.32777938]]\n",
      "[[0.6718788  0.32812116]]\n",
      "[[0.19179852 0.8082015 ]]\n",
      "[[0.65516734 0.34483263]]\n",
      "[[0.6937702  0.30622983]]\n",
      "[[0.70844746 0.29155257]]\n",
      "[[0.6724739  0.32752603]]\n",
      "[[0.7220056  0.27799436]]\n",
      "[[0.15894675 0.84105325]]\n",
      "[[0.6841837  0.31581634]]\n",
      "[[0.6729183 0.3270817]]\n",
      "[[0.06213649 0.9378635 ]]\n",
      "[[0.18882532 0.8111747 ]]\n",
      "[[0.12518978 0.8748103 ]]\n",
      "[[0.6719526 0.3280474]]\n",
      "[[0.695988 0.304012]]\n",
      "[[0.0440205  0.95597947]]\n",
      "[[0.6914917  0.30850822]]\n",
      "[[0.6715978  0.32840222]]\n",
      "[[0.6908828  0.30911717]]\n",
      "[[0.34987104 0.650129  ]]\n",
      "[[0.672885   0.32711506]]\n",
      "[[0.73726064 0.2627394 ]]\n",
      "[[0.28956535 0.7104347 ]]\n",
      "[[0.67236567 0.32763436]]\n",
      "[[0.70005816 0.2999418 ]]\n",
      "[[0.51707834 0.48292166]]\n",
      "[[0.03950238 0.9604976 ]]\n",
      "[[0.6729022  0.32709777]]\n",
      "[[0.05270588 0.9472942 ]]\n",
      "[[0.67311096 0.32688907]]\n",
      "[[0.6938847  0.30611524]]\n",
      "[[0.69627684 0.30372316]]\n",
      "[[0.6726146 0.3273855]]\n",
      "[[0.69301116 0.30698875]]\n",
      "[[0.68581116 0.31418884]]\n",
      "[[0.7218739  0.27812615]]\n",
      "[[0.7089188 0.2910812]]\n",
      "[[0.12502925 0.87497073]]\n",
      "[[0.51307815 0.48692182]]\n",
      "[[0.22148003 0.77852   ]]\n",
      "[[0.6906628  0.30933717]]\n",
      "[[0.6718861 0.3281139]]\n",
      "[[0.37099555 0.6290044 ]]\n",
      "[[0.7280892 0.2719108]]\n",
      "[[0.6727823  0.32721776]]\n",
      "[[0.70437 0.29563]]\n",
      "[[0.70595264 0.29404733]]\n",
      "[[0.0119143 0.9880856]]\n",
      "[[0.11659072 0.88340926]]\n",
      "[[0.67105687 0.32894316]]\n",
      "[[0.67453384 0.32546616]]\n",
      "[[0.69197655 0.30802348]]\n",
      "[[0.6865477  0.31345224]]\n",
      "[[0.6978737 0.3021263]]\n",
      "[[0.6920307  0.30796933]]\n",
      "[[0.67127776 0.32872218]]\n",
      "[[0.6962257 0.3037742]]\n",
      "[[0.6703755  0.32962447]]\n",
      "[[0.7217157  0.27828437]]\n",
      "[[0.6721437 0.3278563]]\n",
      "[[0.70171267 0.29828733]]\n",
      "[[0.67298806 0.32701194]]\n",
      "[[0.68022984 0.31977013]]\n",
      "[[0.6715384  0.32846162]]\n",
      "[[0.67216486 0.3278351 ]]\n",
      "[[0.6501442 0.3498558]]\n",
      "[[0.6932937  0.30670637]]\n",
      "[[0.21314831 0.78685164]]\n",
      "[[0.66638684 0.33361322]]\n",
      "[[0.6132061 0.3867939]]\n",
      "[[0.7039828 0.2960172]]\n",
      "[[0.6722694 0.3277306]]\n",
      "[[0.6881706  0.31182936]]\n",
      "[[0.70363486 0.29636514]]\n",
      "[[0.6774764  0.32252353]]\n",
      "[[0.7035359  0.29646403]]\n",
      "[[0.67175716 0.32824287]]\n",
      "[[0.11653375 0.88346624]]\n",
      "[[0.11617564 0.8838244 ]]\n",
      "[[0.6893212  0.31067875]]\n",
      "[[0.6354001  0.36459988]]\n",
      "[[0.70993733 0.29006267]]\n",
      "[[0.68845814 0.31154186]]\n",
      "[[0.67308295 0.32691702]]\n",
      "[[0.67366606 0.32633394]]\n",
      "[[0.6761147  0.32388535]]\n",
      "[[0.6725801  0.32741994]]\n",
      "[[0.71620363 0.28379634]]\n",
      "[[0.70624053 0.2937595 ]]\n",
      "[[0.6710785 0.3289215]]\n",
      "[[0.70691144 0.29308853]]\n",
      "[[0.6728813  0.32711866]]\n",
      "[[0.726532 0.273468]]\n",
      "[[0.7043372  0.29566282]]\n",
      "[[0.08673175 0.9132682 ]]\n",
      "[[0.67075324 0.32924673]]\n",
      "[[0.30953178 0.6904682 ]]\n",
      "[[0.7142328 0.2857672]]\n",
      "[[0.6723918 0.3276082]]\n",
      "[[0.3860739  0.61392605]]\n",
      "[[0.67912906 0.32087097]]\n",
      "[[0.70398706 0.2960129 ]]\n",
      "[[0.68352014 0.31647992]]\n",
      "[[0.27758628 0.7224138 ]]\n",
      "[[0.67169213 0.32830787]]\n",
      "[[0.6928195 0.3071806]]\n",
      "[[0.5785206 0.4214794]]\n",
      "[[0.6538746  0.34612542]]\n",
      "[[0.6420425  0.35795748]]\n",
      "[[0.6725934  0.32740662]]\n",
      "[[0.7300982  0.26990178]]\n",
      "[[0.6724967 0.3275033]]\n",
      "[[0.13520102 0.86479896]]\n",
      "[[0.69723934 0.3027607 ]]\n",
      "[[0.7161103  0.28388974]]\n",
      "[[0.67203784 0.32796216]]\n",
      "[[0.72079843 0.2792016 ]]\n",
      "[[0.67384124 0.3261587 ]]\n",
      "[[0.67228025 0.3277198 ]]\n",
      "[[0.6830197  0.31698033]]\n",
      "[[0.70348394 0.29651603]]\n",
      "[[0.7014285  0.29857156]]\n",
      "[[0.6841935  0.31580648]]\n",
      "[[0.18521722 0.81478274]]\n",
      "[[0.6909162  0.30908385]]\n",
      "[[0.67226326 0.3277367 ]]\n",
      "[[0.6990986  0.30090138]]\n",
      "[[0.70694155 0.29305848]]\n",
      "[[0.6954168  0.30458316]]\n",
      "[[0.10027669 0.8997233 ]]\n",
      "[[0.68795395 0.312046  ]]\n",
      "[[0.29176646 0.7082335 ]]\n",
      "[[0.12548196 0.87451804]]\n",
      "[[0.6794128  0.32058722]]\n",
      "[[0.38989186 0.61010814]]\n",
      "[[0.6729567 0.3270433]]\n",
      "[[0.51781845 0.48218155]]\n",
      "[[0.6527552 0.3472448]]\n",
      "[[0.6744385 0.3255615]]\n",
      "[[0.68679315 0.31320688]]\n",
      "[[0.6726977  0.32730228]]\n",
      "[[0.6851181 0.3148819]]\n",
      "[[0.1907526  0.80924743]]\n",
      "[[0.09626151 0.90373844]]\n",
      "[[0.73951924 0.26048076]]\n",
      "[[0.6735824  0.32641765]]\n",
      "[[0.6779927  0.32200724]]\n",
      "[[0.6726248  0.32737523]]\n",
      "[[0.6442148  0.35578528]]\n",
      "[[0.67366135 0.32633862]]\n",
      "[[0.43327382 0.5667262 ]]\n",
      "[[0.30164543 0.6983546 ]]\n",
      "[[0.6730885  0.32691142]]\n",
      "[[0.672537 0.327463]]\n",
      "[[0.67296445 0.32703558]]\n",
      "[[0.6708618 0.3291382]]\n",
      "[[0.14614403 0.85385597]]\n",
      "[[0.65821445 0.34178558]]\n",
      "[[0.25867847 0.74132156]]\n",
      "[[0.67199665 0.32800332]]\n",
      "[[0.6715206 0.3284794]]\n",
      "[[0.67282724 0.32717282]]\n",
      "[[0.49605834 0.5039416 ]]\n",
      "[[0.6720127  0.32798734]]\n",
      "[[0.6714462  0.32855374]]\n",
      "[[0.6708922 0.3291078]]\n",
      "[[0.6728505 0.3271495]]\n",
      "[[0.06556195 0.934438  ]]\n",
      "[[0.35146904 0.64853096]]\n",
      "[[0.69426155 0.3057385 ]]\n",
      "[[0.6970887  0.30291128]]\n",
      "[[0.67208517 0.3279148 ]]\n",
      "[[0.70169544 0.2983046 ]]\n",
      "[[0.7079818 0.2920182]]\n",
      "[[0.1275113 0.8724887]]\n",
      "[[0.6719123  0.32808772]]\n"
     ]
    }
   ],
   "source": [
    "X = np.empty((1, SEQUENCE_LENGTH, 4))\n",
    "results = []\n",
    "\n",
    "for i in range(len(eng_data)):\n",
    "    X[0,:,:] = eng_data[i].values\n",
    "    prediction = model.predict(X)\n",
    "    f = 1-np.argmax(prediction[0])\n",
    "    results.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res['engine_no'] = list(range(0,707))\n",
    "\n",
    "r = np.zeros(707)\n",
    "for i in range(len(eng_numb)):\n",
    "    r[eng_numb[i]] = results[i]\n",
    "\n",
    "rr = np.asarray(r, dtype=int)\n",
    "res['result'] = rr\n",
    "res.to_csv('submission.csv', sep=',', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
